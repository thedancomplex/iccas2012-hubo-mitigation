\section{Methodology}
\label{sec:methodology}
Our proposed process is divided up into two distinct parts, failure state
determination (Section\ref{sub:FailureStateDetermination}) and mitigation
(Section\ref{sub:mitigationanalysis}).  The determination techniques are
designed to be wide in scope.  The mitigation techniques are tailored towards
adult-size humanoid robots, specifically the Hubo platforms described in
Section~\ref{sub:platform}.

\begin{table*}[t]
\caption{- Actuator metrics analyzed for convex hull creation}\label{table:Astates}
\begin{center}
\begin{tabular}{|l|c|l|}\hline
 Metric   &   Type   &   Status/Measurement \\\hline\hline
 Over Current 								& Actuator 							& Status flag - motor controller has been shutdown due to over current				\\\hline
 Status												& Actuator							&	Status flag - motor controller's internal PID control loop is activated 		\\\hline
 Zeroed 											& Actuator							& Status flag - motor controller has been zeroed			 												\\\hline
 Responding 									&	Actuator							&	Response - Responds if the motor controller is communicating properly 			\\\hline
 Commanded Position 					&	Actuator							& Reference position given to motor controller.																\\\hline
 Measured Position 						& Actuator							& Actual position of motor 																										\\\hline
 Position Error								& Actuator							& Position error between commanded and measured position of the actuator			\\\hline
 Current											& Actuator							& Current used by the given actuator 																					\\\hline
\end{tabular}
\end{center}
\end{table*}

\subsection{Platform}
\label{sub:platform}
The robots that will be used in these experiments are the adult-size humanoid
robots Jaemi Hubo and Hubo2+, see Fig.~\ref{fig:huboSch}.  Each Hubo contains
40 degrees of freedom (DOF) and stands at a height of 130cm.  Each actuator has
a high gain internal PID position control loop.  Reference positions are
commanded at 100hz over a half-duplex 1.0Mbps controlled area network (CAN).
Each actuator can feed back actual position, current through the actuator, and
the actuator status (enabled or disabled).  Other metrics that are fed back are
described in Table~\ref{table:Astates} and Table~\ref{table:Sstates}.

Currently we have direct access to seven Hubo's in total, six Hubo2+ and one
Hubo2 (Jaemi Hubo).  Two of these robots will be used in these experiments for
initial testing.  Access to this common platform (Hubo) allows us to verify results
with minimal mechatronic differences.  This is the driving goal behind the NSF-MIR2 
(Grant Number CNS-0960061) through which the Hubo platforms were procured.


\subsection{Failure State Determination}
\label{sub:FailureStateDetermination}
To accurately detect when a fault is occurring in the Hubo robots, the Aniketos
system requires an understanding of each robot's normal state. This normal
state is constructed from a variety of safe routines that exercise the
different features of the robots individually and simultaneously. If a fault is
triggered during training, then any subsequent time the fault manifests, it
will be presumed to be safe. If these routines do not exercise all features of
the robots, then it is possible that a mitigation will be triggered when no
fault is occurring. To construct the state, Aniketos collects data from all
available sensors, treating each set of measurements as a point in
$n$-dimensional space. Aniketos uses an online algorithm to construct the
enclosing $n$-dimensional convex hull around these points as each each data
point arrives.

During the monitoring phase, a new measurement point $x$ is a member of the
convex set $X = \{x_1, \dots, x_k \}$ if

\begin{equation}
\hfill	x = \sum_{i=1}^{k} \lambda_ix_i \hfill
\end{equation}

In order to satisfy the convex hull criteria
\begin{equation}
\hfill	\lambda_i \geq 0 \hfill
\end{equation}

and

\begin{equation}
\hfill	\sum_{i=1}^{k} \lambda_i = 1 \hfill
\end{equation}

This can be solved using a nonnegative linear least squares
algorithm\cite{Lawson1974}. If a feasible solution exists, then the point is
contained in the convex set. The time needed to classify a single measurement
point is dependent on the number of dimensions and the number of points needed
to define the hull.

Aniketos determines that a fault is occurring when the current system state is
outside of the normal state. To diagnose that a specific is occurring fault
hulls must be created for each known possible fault. This fault data is
collected in a fashion similar to the normal state, except the fault must be
triggered in a controlled environment. Section~\ref{sec:faultInjection} describes the
faults that will be studied and how
these faults are injected into the system in a lab setting.

Once all of the hulls are constructed, the monitoring phase will analyze each
measurement point and determine whether the robot is operating normally. If it
is not, Aniketos will attempt to determine if a known fault is occurring, or if
an unknown fault is occurring.  Typically, it takes less than 100ms to classify
a point using 25 dimensions and a hull that is constructed from 10000 points using a single threaded 2.4Ghz x86 computer.
To better meet the real-time data rate, algorithms may be used to reduce the
number of points, and more efficient algorithm implements can be used to reduce
the processing time. If a fault is detected, an appropriate mitigation should
be selected and applied.




\begin{table*}[t]
\caption{- Sensor and kinematic metrics analyzed for convex hull creation}\label{table:Sstates}
\begin{center}
\begin{tabular}{|l|c|l|}\hline
 Metric   &   Type   &   Status/Measurement \\\hline\hline
 Orientation (CG)							& Sensor								& Orientation in 3-DOF given by the IMU																				\\\hline
 Force Torque (feet)					& Sensor								& Force-torque measurement in X,Y, and Z directions on each ankle							\\\hline
 Force Torque (hands)					& Sensor								& Force-torque measurement in X,Y, and Z directions on each wrist							\\\hline
 Orientation (feet)						& Sensor								& 2-DOF orientation (X,Y) of each of the feet 																\\\hline
 ZMP Location									& Kinematics						& Current ZMP location in (X,Y,Z)																							\\\hline
\end{tabular}
\end{center}
\end{table*}

\subsection{Fault Injection}
\label{sec:faultInjection}
Faults will be injected to the Hubo system in a lab setting.  Each fault will
be injected on eight separate trials to ensure statistically significant
results.  During each test system metrics are be recorded at a rate of 100hz
for the entirety of the test.  Table~\ref{table:Astates} and Table~\ref{table:Sstates} describes each of the
metrics recorded.  Each test will consist of two stages.  The first stage lasts
30 seconds.  In this stage the robot is standing or walking (depending on the
test) in a stable fault and failure free state.  The given fault is injected at
30 seconds.  This marks the beginning of the second stage.  The second stage
will last two minutes or 30 seconds after instability has occurred, which ever
comes first.

The faults that will be injected are \textit{actuator failure} and \textit{ZMP loss}.
Actuator failures will be created by:

\begin{itemize}
\item Removing power to actuator during operation (Actuator will no longer respond)
\item Removing power to motor during operation (Actuator will respond but the motor has no power)
\item Lowering the over current threshold within the actuator causing an over current shutdown.
\end{itemize}

\noindent ZMP loss will be tested by:
\begin{itemize}
\item Known impulse (push) applied to shoulder in the x and y directions.
\item Step onto un-even terrain.
\item Sudden ground shift in x and y direction (treadmill turned on then off)
\end{itemize}

To ensure no physical harm to the robot during testing, All test will be preformed
in the full-scale safe testing environment designed for experiments with Jaemi Hubo
created using DASL’s Systems Integrated Sensor Test Rig (SISTR)\cite{5686325}.

A $n$-dimensional convex hull for the normal operating state and each of the injected fault states
will be created using the above data using the methods described in Section~\ref{sub:FailureStateDetermination}.

\subsection{Mitigation Analysis}
\label{sub:mitigationanalysis}
The effectiveness of different mitigations has been analyzed on faults injected
into two different software systems typically used as benchmark applications in
the software engineering community, RUBiS\footnote{http://rubis.ow2.org/} and
Hadoop\footnote{http://hadoop.apache.org/}. RUBiS is a web application auction
site running on the Apache Tomcat\footnote{http://tomcat.apache.org/} web
server and serves page requests to hundreds of concurrently simulated
clients. Hadoop is a distributed task manager that processes gigabytes of data
across multiple nodes. We injected faults into both systems, the virtual
machines they are running on, as well as the hosts managing the virtual
machines. The faults consumed either processor, memory, disk, database, or
network resources in the respective components. We applied generic mitigations
that restarted various components, migrated the components to new hosts or
virtual machines, or did nothing. The effectiveness of each fault-mitigation
pair was analyzed on each system to generate a mapping of faults to
mitigations. Using this knowledge, if a fault is later encountered, the best
mitigation can be applied with a high chance of success. If a particular
mitigation cannot be applied in that instance, then the next best mitigation
may be selected, and so forth.

A naive observer would expect the two systems to have nearly identical
fault-mitigation mappings. However, in our testing on software based systems
the generated mappings varied greatly in the effectiveness of the mitigations
on a particular fault.  RUBiS is a typical web application that receives
requests from clients and processes results returned from a database. Little
processing is being done by RUBiS or its host machine, so even though a
processor intensive fault may be using up many cycles in the process or the
host, there is no noticeable effect on the request processing rate. As a
result, any mitigation applied would result in more down time than if no
mitigation were to be applied. Unlike RUBiS, Hadoop is much more processor
intensive. Any host level fault that is slowing down the system will adversely
affect the completion time for the task. Another major difference is that
RUBiS, like many other request driven systems, can easily be restarted to
temporarily address fault symptoms, while restarting Hadoop typically means
losing already processed results.  We expect similar results when applying this
method to the complex electro-mechanical platforms Jaemi Hubo/Hubo2+.

The generated fault-mitigation mappings for the latter two software systems
offer insight into their respective natures. On the surface, both software
systems operate like typical servers, waiting for requests or tasks, and
processing them in a timely fashion. However, the way the requests are handled
results in faults manifesting in different ways. Even though a fault may be
detected, the obvious mitigation is not always the best. This approach to
mitigation analysis, will offer valuable insight into the true effectiveness of
mitigations when faults occur in Hubo.



\subsection{Hubo Mitigations}
\label{sub:hubomitigations}
As in the software case studies, no single mitigation will correct all
faults. The effect of each mitigation on a specific fault should be analyzed
based on execution time, effectiveness (time to next fault), and risk of damage
to the system.
