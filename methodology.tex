\section{Methodology}

Our proposed process is divided up into two distinct parts, failure state 
determination - Section~\ref{sub:FailureStateDetermination}~~ and mitigation - Section~\ref{sub:mitigationanalysis}~~.  The determination techniques designed to be 
wide in scope.  The midigation techniques are tailured towards adult-size
humanoid robots, specifically the Hubo platforms described in Section~\ref{sec:platform}.

 
\subsection{Platform}\label{sec:platform}
The robots that will be used in these experiments are the adult-size 
humanoid robots Jaemi Hubo and Hubo2+, see Fig.~\ref{fig:huboSch}.  
Each Hubo contains 40 degrees of freedom (DOF) and stands at a height of 130cm.
Each actuator has a high gain internal PID position control loop.  Reference positions are
commanded at 100hz over a half-duplex 1.0Mbps controlled area network (CAN).
Each actuator can feed back actual position, current through the actuator, and the actuator status 
(enabled or disabled).  Other metrics that are fed back are described in 
Table~\ref{table:states}.  


Currently we have direct access to seven Hubo's in total, six Hubo2+ and one Hubo2 (Jaemi Hubo).
Two of these robots will be used in these experiments for initial testing.



\subsection{Failure State Determination}
\label{sub:FailureStateDetermination}
To accurately detect when a fault is occurring in the Hubo robots, the Aniketos system requires an understanding of each robot's normal state. This normal state is constructed from a variety of safe routines that exercise the different features of the robots individually and simultaneously. If a fault is triggered during training, then any subsequent time the fault manifests, it will be presumed to be safe. If these routines do not exercise all features of the robots, then it is possible that a mitigation will be triggered when no fault is occurring. To construct the state, Aniketos collects data from all available sensors, treating each set of measurements as a point in $n$-dimensional space. Aniketos uses an online algorithm to construct the enclosing $n$-dimensional convex hull around these points as each each data point arrives. The time needed to classify a single measurement point is dependent on the number of dimensions and the number of points needed to define the hull.


Aniketos determines that a fault is occurring when the current system state is outside of the normal state. To diagnose that a specific is occuring fault hulls must be created for each known possible fault. This fault data is collected in a fashion similar to the normal state, except the fault must be triggered in a controlled environment. Section~\ref{} describes the faults that will be studied.

Once all of the hulls are constructed, the monitoring phase will analyze each measurement point and determine whether the robot is operating normally. If it is not, Aniketos will attempt to determine if a known fault is occurring, or if an unknown fault is occurring. Typically, it takes less than 100ms to classify a point using 25 dimensions and a hull that is constructed from 10000 points. To better meet the real-time data rate, algorithms may be used to reduce the number of points, and more efficient algorithm implements can be used to reduce the processing time. If a fault is detected, an appropriate mitigation should be selected and applied.

\subsection{Mitigation Analysis}
\label{sub:mitigationanalysis}
The effectiveness of different mitigations has been analyzed on faults injected into two different software systems typically used as benchmark applications in the software engineering community, RUBiS\footnote{http://rubis.ow2.org/} and Hadoop\footnote{http://hadoop.apache.org/}. RUBiS is a web application auction site running on the Apache Tomcat\footnote{http://tomcat.apache.org/} web server and serves page requests to hundreds of concurrently simulated clients. Hadoop is a distributed task manager that processes gigabytes of data across multiple nodes. We injected faults into both systems, the virtual machines they are running on, as well as the hosts managing the virtual machines. The faults consumed either processor, memory, disk, database, or network resources in the respective components. We applied generic mitigations that restarted various components, migrated the components to new hosts or virtual machines, or did nothing. The effectiveness of each fault-mitigation pair was analyzed on each system to generate a mapping of faults to mitigations. Using this knowledge, if a fault is later encountered, the best mitigation can be applied with a high chance of success. If a particular mitigation cannot be applied in that instance, then the next best mitigation may be selected, and so forth.


A naive observer would expect the two systems to have nearly identical fault-mitigation mappings. However, in our testing on software based systems the generated mappings varied greatly in the effectiveness of the mitigations on a particular fault.  RUBiS is a typical web application that receives requests from clients and processes results returned from a database. Little processing is being done by RUBiS or its host machine, so even though a processor intensive fault may be using up many cycles in the process or the host, there is no noticeable effect on the request processing rate. As a result, any mitigation applied would result in more down time than if no mitigation were to be applied. Unlike RUBiS, Hadoop is much more processor intensive. Any host level fault that is slowing down the system will adversely affect the completion time for the task. Another major difference is that RUBiS, like many other request driven systems, can easily be restarted to temporarily address fault symptoms, while restarting Hadoop typically means losing already processed results.  We expect similar results when applying this method to the complex electro-mechanical platforms Jaemi Hubo/Hubo2+.

The generated fault-mitigation mappings for the two software systems offer insight into their respective natures. On the surface, both software systems operate like typical servers, waiting for requests or tasks, and processing them in a timely fashion. However, the way the requests are handled results in faults manifesting in different ways. Even though a fault may be detected, the obvious mitigation is not always the best. This approach to mitigation analysis, will offer valuable insight into the true effectiveness of mitigations when faults occur in Hubo.

\subsection{Hubo Fault Injection}
Faults will be injected to the Hubo system in a lab setting.  Each fault will be injected on eight separate trials to ensure statistically significant results.  During each system metrics are be recorded at a rate of 100hz for the entirety of the test.  Table~\ref{table:states} discribes each of the metrics recorded.  Each test will consist of two stages.  The first stage lasts 30 seconds.  In this stage the robot standing or walking (depending on the test) in a stable fault free state.  The fault is injected at 30 seconds.  This marks the beginning of the second stage.  The second stage will last two minutes or 30 seconds after instability has occurred, which ever comes first.

\subsection{Hubo Mitigations}
As in the software case studies, no single mitigation will correct all faults. The effect of each mitigation on a specific fault should be analyzed based on execution time, effectiveness (time to next fault), and risk of damage to the system.







